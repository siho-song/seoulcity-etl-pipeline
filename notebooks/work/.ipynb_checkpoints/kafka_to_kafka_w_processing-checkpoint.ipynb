{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from IPython.display import display, display_pretty, clear_output, JSON\n",
    "# 공통 데이터 위치\n",
    "home_jovyan = \"/home/jovyan\"\n",
    "work_data = f\"{home_jovyan}/work/data\"\n",
    "work_dir=!pwd\n",
    "work_dir = work_dir[0]\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# 노트북에서 테이블 형태로 데이터 프레임 출력을 위한 설정을 합니다\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # display enabled\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 100) # display output columns size\n",
    "\n",
    "# 로컬 환경 최적화\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5) # the number of partitions to use when shuffling data for joins or aggregations.\n",
    "spark.conf.set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name = \"rawdata\"\n",
    "kafkaReader = (\n",
    "    spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9093\")\n",
    "  .option(\"subscribe\", topic_name)\n",
    "  .option(\"startingOffsets\", \"earliest\")\n",
    "  .load()\n",
    ")\n",
    "\n",
    "\n",
    "kafkaSchema = (\n",
    "    StructType()\n",
    "    .add(StructField(\"status\", StringType()))\n",
    "    .add(StructField(\"id\", StringType()))\n",
    "    .add(StructField(\"screen_name\", StringType()))\n",
    "    .add(StructField(\"user_id\", IntegerType()))\n",
    "    .add(StructField(\"profile\", StringType()))\n",
    "    .add(StructField(\"time\", DateType()))\n",
    "    .add(StructField(\"text\", StringType()))\n",
    "    .add(StructField(\"retweet_count\", IntegerType()))\n",
    "    .add(StructField(\"favorite_count\", IntegerType()))\n",
    "    .add(StructField(\"lat\", IntegerType()))\n",
    "    .add(StructField(\"long\", IntegerType()))\n",
    ")\n",
    "\n",
    "\n",
    "kafkaSelector = (\n",
    "    kafkaReader\n",
    "    .select(\n",
    "        col(\"key\").cast(\"string\"),\n",
    "        from_json(col(\"value\").cast(\"string\"), kafkaSchema).alias(\"rawtweet\")\n",
    "    )\n",
    "    .selectExpr(\"rawtweet.id as key\", \"rawtweet.status\", \"rawtweet.screen_name\", \"rawtweet.user_id\", \"rawtweet.profile\", \"rawtweet.time\", \"rawtweet.text\", \"rawtweet.retweet_count\", \"rawtweet.favorite_count\", \"rawtweet.lat\", \"rawtweet.long\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글, 영어, 숫자, 모든 특수문자, 이모지를 제외한 나머지 (중국어, 일본어, 태국어 등)를 포함하고있다면\n",
    "# None리턴, 아닐시 s 리턴\n",
    "def processing(s):\n",
    "    import re\n",
    "    pattern = re.compile('[^ ㄱ-ㅣ가-힣a-zA-Z0-9\\nu\"\\U00010000-\\U0010FFFF\"[\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\"]+', flags=re.UNICODE)\n",
    "    if len(pattern.findall(s)) > 10:\n",
    "        return None\n",
    "    else:\n",
    "        return s\n",
    "processing_udf = udf(lambda x: processing(x),StringType())\n",
    "\n",
    "# 어떤 그룹에 대한 데이터인지 파악\n",
    "def find_group(s):\n",
    "    s = s.lower()\n",
    "    if 'blackpink' in s:\n",
    "        return 'blackpink'\n",
    "    elif 'bts' in s:\n",
    "        return 'bts'\n",
    "    else:\n",
    "        return None\n",
    "find_group_udf = udf(lambda x: find_group(x), StringType())\n",
    "\n",
    "kafkaSelector = (\n",
    "    kafkaSelector\n",
    "    .where(\"favorite_count>0\")\n",
    "    .withColumn(\"processed_text\", processing_udf(col(\"text\")))\n",
    "    .withColumn(\"group\", find_group_udf(col(\"text\")))\n",
    ")\n",
    "\n",
    "p_kafkaSelector = (\n",
    "    kafkaSelector\n",
    "    .withColumn(\"value\", to_json(struct(\"screen_name\", \"profile\", \"time\", \"processed_text\", \"retweet_count\", \"favorite_count\", \"group\")))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write to kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayStatus(name, query, iterations, sleep_secs):\n",
    "    from time import sleep\n",
    "    i = 1\n",
    "    for x in range(iterations):\n",
    "        clear_output(wait=True)      # Output Cell 의 내용을 지웁니다\n",
    "        display('[' + name + '] Iteration: '+str(i)+', Status: '+query.status['message'])\n",
    "        display(query.lastProgress)  # 마지막 수행된 쿼리의 상태를 출력합니다\n",
    "        sleep(sleep_secs)            # 지정된 시간(초)을 대기합니다\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[kafkaQ] Iteration: 66, Status: Processing new data'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': '66ebca71-a511-4b1e-ae2c-667273a2c6e4',\n",
       " 'runId': 'b098a77b-cd3a-43ca-9221-67ee2aa8de82',\n",
       " 'name': 'kafkaQ',\n",
       " 'timestamp': '2022-12-15T13:46:05.000Z',\n",
       " 'batchId': 63,\n",
       " 'numInputRows': 1,\n",
       " 'inputRowsPerSecond': 0.20004000800160032,\n",
       " 'processedRowsPerSecond': 1.3297872340425532,\n",
       " 'durationMs': {'addBatch': 610,\n",
       "  'getBatch': 0,\n",
       "  'latestOffset': 3,\n",
       "  'queryPlanning': 21,\n",
       "  'triggerExecution': 752,\n",
       "  'walCommit': 50},\n",
       " 'stateOperators': [],\n",
       " 'sources': [{'description': 'KafkaV2[Subscribe[rawdata]]',\n",
       "   'startOffset': {'rawdata': {'0': 88}},\n",
       "   'endOffset': {'rawdata': {'0': 89}},\n",
       "   'latestOffset': {'rawdata': {'0': 89}},\n",
       "   'numInputRows': 1,\n",
       "   'inputRowsPerSecond': 0.20004000800160032,\n",
       "   'processedRowsPerSecond': 1.3297872340425532,\n",
       "   'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
       "    'maxOffsetsBehindLatest': '0',\n",
       "    'minOffsetsBehindLatest': '0'}}],\n",
       " 'sink': {'description': 'org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@70afd515',\n",
       "  'numOutputRows': 1}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qname = \"kafkaQ\"\n",
    "kafkaWriter_origin = (\n",
    "    p_kafkaSelector.select(\"key\", \"value\")\n",
    "    .writeStream\n",
    "    .queryName(qname)\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9093\")\n",
    "    .option(\"topic\", \"processed\")\n",
    "    .outputMode(\"append\")\n",
    ")\n",
    "checkpointLocation = f\"{work_dir}/tmp/{qname}\"\n",
    "!rm -rf $checkpointLocation\n",
    "kafkaTrigger = (\n",
    "    kafkaWriter_origin\n",
    "    .trigger(processingTime=\"5 second\")\n",
    "    .option(\"checkpointLocation\", checkpointLocation)\n",
    ")\n",
    "\n",
    "kafkaQuery = kafkaTrigger.start()\n",
    "\n",
    "displayStatus(qname, kafkaQuery, 1000, 5)\n",
    "    \n",
    "kafkaQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}